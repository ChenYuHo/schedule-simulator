from schedule_simulator_core.DAGs import Layer, DAG
from model_extraction.keras_utils import traverse_keras_DFS, get_layer_children, get_layer_parents
from schedule_simulator_core.DAGs import LOCAL_EXTRA_PREFIX
import numpy as np


def keras_model_to_DAG(model, skipped_layer_types=None):
    input_layers = list()
    all_layers = dict()
    skipped_layers_connections = dict()  # Needed to forward connections

    def skip(keras_layer):
        for skipped_layer_type in skipped_layer_types:
            if isinstance(keras_layer, skipped_layer_type):
                return True
        return False

    # Skip input layers by default
    if skipped_layer_types is None:
        from tensorflow.python.keras.layers import InputLayer
        skipped_layer_types = [InputLayer]

    i = 0

    def add_layer(keras_layer):
        nonlocal i
        if skip(keras_layer):
            connections = {"parents": [x.name for x in get_layer_parents(keras_layer)],
                           "children": [x.name for x in get_layer_children(keras_layer)]}
            skipped_layers_connections[keras_layer.name] = connections
            return
        param_count = keras_layer.count_params()
        comm_units = param_count * 4  # Each parameter is a 4 bytes
        comp_units = param_count * 4
        sim_layer = Layer(comp_units, comp_units, comm_units, name=keras_layer.name, type=type(keras_layer).__name__,
                          index=i)
        all_layers[keras_layer.name] = sim_layer
        i += 1

    def connect_layer(keras_layer):
        if skip(keras_layer):
            return
        sim_layer = all_layers[keras_layer.name]
        sim_layer.input_layers = list()
        sim_layer.output_layers = list()

        def add_parents_after_skipping(keras_layer, parents_set):
            for parent in get_layer_parents(keras_layer):
                if skip(parent):
                    add_parents_after_skipping(parent, parents_set)
                else:
                    parents_set.add(parent)

        def add_children_after_skipping(keras_layer, children_set):
            for child in get_layer_children(keras_layer):
                if skip(child):
                    add_children_after_skipping(child, children_set)
                else:
                    children_set.add(child)

        keras_parents = set()
        add_parents_after_skipping(keras_layer, keras_parents)
        keras_children = set()
        add_children_after_skipping(keras_layer, keras_children)
        if len(keras_parents) == 0:
            input_layers.append(sim_layer)
        for keras_parent_layer in keras_parents:
            sim_layer.input_layers.append(all_layers[keras_parent_layer.name])
        for keras_child_layer in keras_children:
            sim_layer.output_layers.append(all_layers[keras_child_layer.name])

    for fun in [add_layer, connect_layer]:
        traverse_keras_DFS(model, processing_function=fun, order="pre-order", top_to_bottom=True)
    units = {"{}comm_unit".format(LOCAL_EXTRA_PREFIX): "B", "{}forward_pass_unit".format(LOCAL_EXTRA_PREFIX): "B",
             "{}backward_pass_unit".format(LOCAL_EXTRA_PREFIX): "B", }
    return DAG(input_layers, name=model.name, **units)


def extract_costs_from_model_reconstruct_profile(profiling_report, suppress_negatives=0, reduce_func=None):
    """
    :param profiling_report: a report generated by profiling a keras model using the
    keras_model_reconstruct_layerwise_costs_profiler.py tool
    :param suppress_negatives: False | after | before
    0: values are left as is
    1: Negative values are left to affect next layer calculation however it is recorded as a 0 at the end
    2: Negative values set to 0 before being sent to the next layer so that it does not affect next layers.
    :return: A dict(key=layer.name, value=dict(key=cost_name, value=COST))
    """
    timings = profiling_report["timings"]
    scaling_factor = 1/profiling_report["args"]["batch_size"]
    layer_costs = dict()
    accumulative_cost = {"forward_pass_units": 0, "backward_pass_units": 0}
    for layer_name in timings:
        evaluate = timings[layer_name]["evaluate"]
        fit = timings[layer_name]["fit"]
        if isinstance(evaluate, dict):
            evaluate = evaluate["durations"]
            fit = fit["durations"]
        if reduce_func is None:
            reduce = lambda x: np.min(x)*scaling_factor
        else:
            reduce = lambda x: reduce_func(x)*scaling_factor
        fit = reduce(fit)
        evaluate = reduce(evaluate)
        current_cost = dict()
        current_cost["forward_pass_units"] = evaluate - accumulative_cost["forward_pass_units"]
        current_cost["backward_pass_units"] = fit - evaluate - accumulative_cost["backward_pass_units"]
        if suppress_negatives == 1:
            accumulative_cost["forward_pass_units"] += current_cost["forward_pass_units"]
            accumulative_cost["backward_pass_units"] += current_cost["backward_pass_units"]
        if suppress_negatives > 0:
            if current_cost["forward_pass_units"] < 0:
                print("Suppressing {:18} forward_pass_units  {}".format(layer_name, current_cost["forward_pass_units"]))
                current_cost["forward_pass_units"] = 0
            if current_cost["backward_pass_units"] < 0:
                print("Suppressing {:18} backward_pass_units {}".format(layer_name, current_cost["backward_pass_units"]))
                current_cost["backward_pass_units"] = 0
        if suppress_negatives != 1:
            accumulative_cost["forward_pass_units"] += current_cost["forward_pass_units"]
            accumulative_cost["backward_pass_units"] += current_cost["backward_pass_units"]
        layer_costs[layer_name] = current_cost
    return layer_costs


def extract_costs_from_layer_name_mapping_profile(profiling_report, reduce_func=None,
                                                  skip_first_batch=False):
    layer_costs = profiling_report["layer_costs"]
    scaling_factor = 1/profiling_report["args"]["batch_size"]
    if reduce_func is None:
        reduce = lambda x: np.min(x) * scaling_factor
    else:
        reduce = lambda x: reduce_func(x) * scaling_factor
    for layer_name, cost_dict in layer_costs.items():
        if skip_first_batch:
            for cost_name, cost_list in cost_dict.items():
                for i in range(profiling_report["args"]["trials"]):
                    cost_list.pop(i * profiling_report["args"]["num_of_batches"] - i)
        for cost_name, cost_list in cost_dict.items():
            layer_costs[layer_name][cost_name] = reduce(cost_list)
    return layer_costs


def apply_layer_costs_to_dag(dag, profiling_report, extracted_costs):
    """
    :param dag: The simulator dat to apply the profile to
    :param profiling_report: a report generated by profiling a keras model using the keras_model_reconstruct_layerwise_costs_profiler.py tool
    :param suppress_negatives: passed directly to extract_cost_units_from_profile function
    :param scaling_factor: passed directly to extract_cost_units_from_profile function
    """
    def apply_timing(sim_layer: Layer):
        layer_name = sim_layer.extras["name"]
        if layer_name not in extracted_costs:
            print("Skipping layer {} since its costs were not found in the costs report.".format(layer_name))
            return
        layer_timing = extracted_costs[layer_name]
        if "forward_pass_units" in layer_timing:
            dag.extras["{}forward_pass_unit".format(LOCAL_EXTRA_PREFIX)] = "ns"
            sim_layer.forward_pass_units = layer_timing["forward_pass_units"]
        if "backward_pass_units" in layer_timing:
            dag.extras["{}backward_pass_unit".format(LOCAL_EXTRA_PREFIX)] = "ns"
            sim_layer.backward_pass_units = layer_timing["backward_pass_units"]
        if "communication_units" in layer_timing:
            dag.extras["{}comm_unit".format(LOCAL_EXTRA_PREFIX)] = "ns"
            sim_layer.communication_units = layer_timing["communication_units"]
    dag.traverse_BFS(processing_function=apply_timing)


if __name__ == "__main__":
    """
    Example usage
    """
    import json
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers.pooling import Pooling1D, Pooling2D, Pooling3D
    from tensorflow.python.keras.applications import VGG16, ResNet50, DenseNet121, InceptionV3
    from schedule_simulator_core.DAGs import serialize_dag, deserialize_dag
    model = VGG16(weights=None, include_top=True)
    dag = keras_model_to_DAG(model, skipped_layer_types=[InputLayer])
    with open("layer_name_mapping_profiling_reports/VGG16_07-22-17-33.profile.json") as report_file:
        model_profiling_report = json.load(report_file)
    # costs = extract_costs_from_model_reconstruct_profile(model_profiling_report, suppress_negatives=1)
    costs = extract_costs_from_layer_name_mapping_profile(model_profiling_report)
    apply_layer_costs_to_dag(dag, model_profiling_report, costs)
    dag.extras["{}model_profiling_report_name".format(LOCAL_EXTRA_PREFIX)] = "VGG16_07-22-17-33.profile.json"
    dag.extras["{}model_profiling_args".format(LOCAL_EXTRA_PREFIX)] = model_profiling_report["args"]
    with open("dags/VGG16_CPU_lnm.dag", "w") as output_file:
        output_file.write(serialize_dag(dag))

